import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from urllib.parse import parse_qs, urlparse, urlunparse, unquote
import time
import json
import subprocess
import re
import os


def setup_driver():
    options = Options()
    # options.add_argument("--headless=new")  # å¦‚éœ€ç„¡é ­æ¨¡å¼å¯å•Ÿç”¨ï¼Œä½†IGé˜²çˆ¬æ©Ÿåˆ¶ç¹éå¹¾æ¬¡å¾Œæœƒéœ€è¦ç™»å…¥
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # æ¨¡æ“¬ desktop user-agent
    options.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0.0.0 Safari/537.36"
    )

    driver_path = (
        "/Users/shinstar5/chromedriver-mac-x64/chromedriver"  # è«‹æ”¹ç‚ºä½ æœ¬æ©Ÿçš„ chromedriver è·¯å¾‘
    )
    return webdriver.Chrome(service=Service(driver_path), options=options)


def get_site_title(url):
    try:
        resp = requests.get(url, timeout=5)
        soup = BeautifulSoup(resp.text, "html.parser")
        return soup.title.string.strip() if soup.title else None
    except:
        return None


def expand_linkhub(url):
    try:
        resp = requests.get(url, timeout=5)
        soup = BeautifulSoup(resp.text, "html.parser")
        links = []
        for a in soup.find_all("a", href=True):
            href = a["href"]
            title = a.get_text(strip=True) or get_site_title(href) or href
            if href.startswith("http"):
                links.append([title, href])
        return links
    except Exception as e:
        return [["expand_linkhub error", str(e)]]


def extract_youtube_links(url):
    driver = setup_driver()
    links = []
    try:
        if not url.endswith("/about"):
            url = url.rstrip("/") + "/about"
        driver.get(url)

        # ç­‰åˆ°å®¹å™¨è¼‰å…¥
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "link-list-container"))
        )
        time.sleep(2)  # ç­‰å…§éƒ¨ span æ¸²æŸ“å®Œ

        items = driver.find_elements(
            By.CSS_SELECTOR, "#link-list-container yt-channel-external-link-view-model"
        )
        for item in items:
            try:
                href = item.find_element(By.TAG_NAME, "a").get_attribute("href")

                # è‹¥ç‚º redirectï¼Œè§£æå‡ºçœŸå¯¦ URL
                if "youtube.com/redirect" in href:
                    parsed = parse_qs(urlparse(href).query)
                    real_url = unquote(parsed.get("q", [href])[0])
                else:
                    real_url = href

                # å˜—è©¦æŠ“ titleï¼Œè‹¥ä¸å­˜åœ¨å°±ç•¥é
                try:
                    title = item.find_element(
                        By.CSS_SELECTOR,
                        ".yt-channel-external-link-view-model-wiz__title",
                    ).text.strip()
                    if not title:
                        raise ValueError("ç©ºç™½ title")
                except Exception as e:
                    print(f"âš ï¸ title æŠ“ä¸åˆ°ï¼Œç•¥é: {e}")
                    continue

                # âœ… è¨˜å¾— append æ­£ç¢ºè§£æå¾Œçš„ real_urlï¼
                links.append([title, real_url])
            except Exception as e:
                print(f"âš ï¸ å¿½ç•¥éŒ¯èª¤é€£çµ: {e}")
                continue

        return links
    finally:
        driver.quit()


def extract_instagram_links(url):
    driver = setup_driver()
    links = []
    try:
        driver.get(url)
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        # â‘  ç­‰ä¸€ä¸‹ï¼Œæ¨¡æ“¬æ­£å¸¸ä½¿ç”¨è€…
        time.sleep(6)
        driver.get(url)
        driver.refresh()
        time.sleep(3)
        input("ğŸ“ å·²è¼‰å…¥ Instagram é é¢ï¼Œè«‹æª¢æŸ¥ç•«é¢å¾ŒæŒ‰ Enter ç¹¼çºŒï¼Œå¦‚éœ€ç™»å…¥è«‹ç™»å…¥...")

        # â‘¡ å˜—è©¦é»é—œé–‰åœ–ç¤º
        try:
            close_btn = driver.find_element(By.XPATH, "//div[.//svg[@aria-label='é—œé–‰']]")
            close_btn.click()
            print("âœ… æ¨¡æ“¬é»æ“Šé—œé–‰ IG ç™»å…¥ç‰†æˆåŠŸ")
            time.sleep(1)

        except Exception as e:
            print("âš ï¸ æ²’æ‰¾åˆ°é—œé–‰åœ–ç¤ºï¼Œå˜—è©¦é»ç•«é¢å³ä¸‹è§’")

            # â‘¢ é»å³ä¸‹è§’é é›¢ modal çš„åœ°æ–¹ï¼ˆé¿å…èª¤è§¸ç™»å…¥ï¼‰
            try:
                html = driver.find_element(By.TAG_NAME, "html")
                driver.execute_script("window.scrollBy(0, 200);")  # æ»¾å‹•ä¸€é»
                ActionChains(driver).move_to_element_with_offset(
                    html, 20, 700
                ).click().perform()
                print("âœ… é»æ“Šå³ä¸‹é é›¢å€åŸŸæˆåŠŸ")
                time.sleep(1)
            except Exception as e:
                print("âŒ é»å³ä¸‹å¤±æ•—:", e)

        input("ğŸ“ å·²è¼‰å…¥ Instagram é é¢ï¼Œè«‹æª¢æŸ¥ç•«é¢å¾ŒæŒ‰ Enter ç¹¼çºŒï¼Œå¦‚éœ€ç™»å…¥è«‹ç™»å…¥...")

        # ğŸ”¹ 1. å˜—è©¦å±•é–‹ modalï¼ˆå¦‚æœ bio å‡ºç¾ +è™ŸæŒ‰éˆ•ï¼‰
        try:
            expand_button = driver.find_element(By.XPATH, "//button[contains(., '+')]")
            expand_button.click()
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, "//div[@role='dialog']"))
            )
            time.sleep(1)

            a_tags = driver.find_elements(By.XPATH, "//div[@role='dialog']//a")
            for a_tag in a_tags:
                try:
                    href = a_tag.get_attribute("href")
                    spans = a_tag.find_elements(By.TAG_NAME, "span")
                    title = spans[0].text.strip() if spans else "Instagram link"
                    if "l.instagram.com" in href:
                        parsed = parse_qs(urlparse(href).query)
                        raw_url = unquote(parsed.get("u", [href])[0])
                        parsed_url = urlparse(raw_url)
                        real_url = urlunparse(
                            (
                                parsed_url.scheme,
                                parsed_url.netloc,
                                parsed_url.path,
                                "",
                                "",
                                "",
                            )
                        )
                    else:
                        real_url = href
                    links.append([title, real_url])

                except Exception as e:
                    links.append(["parse IG modal link error", str(e)])

            LINKHUB_DOMAINS = [
                "linktr.ee",
                "bio.site",
                "beacons.ai",
                "linkby.tw",
                "campsite.bio",
                "taplink.cc",
            ]
            # åˆ¤æ–· links è£¡æ˜¯å¦æœ‰ link-in-bio å¹³å°ï¼Œé¡å¤–éè¿´æ“´å±•
            for title, real_url in links[:]:  # ç”¨ [:] è¤‡è£½æ¸…å–®é¿å…è¿´åœˆä¸­æ“´å±•éŒ¯ä½
                if any(domain in real_url for domain in LINKHUB_DOMAINS):
                    links.extend(expand_linkhub(real_url))

            return links

        except Exception:
            print("â„¹ï¸ ç„¡ modalï¼Œæ”¹æŠ“ bio å€çŸ­é€£çµ")

        # ğŸ”¹ 2. Fallbackï¼šæŠ“ bio è£¡çš„å–®ä¸€è·³è½‰é€£çµï¼ˆå¦‚ linktr.ee, ppt.ccï¼‰
        try:
            link = driver.find_element(
                By.XPATH, "//a[contains(@href, 'l.instagram.com')]"
            )
            href = link.get_attribute("href")
            parsed = parse_qs(urlparse(href).query)
            raw_url = unquote(parsed.get("u", [href])[0])
            parsed_url = urlparse(raw_url)
            real_url = urlunparse(
                (parsed_url.scheme, parsed_url.netloc, parsed_url.path, "", "", "")
            )

            # æ˜¯ link-in-bio é¡æœå‹™ï¼Œé€²ä¸€æ­¥çˆ¬å…§éƒ¨é€£çµ
            LINKHUB_DOMAINS = [
                "linktr.ee",
                "bio.site",
                "beacons.ai",
                "linkby.tw",
                "campsite.bio",
                "taplink.cc",
            ]
            if any(domain in real_url for domain in LINKHUB_DOMAINS):
                return expand_linkhub(real_url)

            # æ˜¯å–®ä¸€é€£çµï¼ˆå¦‚ bit.ly / ppt.ccï¼‰
            title = get_site_title(real_url)
            links.append([title or "Instagram bio link", real_url])

        except Exception as e:
            links.append(["bio fallback error", str(e)])

        return links

    finally:
        driver.quit()


def get_creator_links(url):
    if "youtube.com" in url:
        return extract_youtube_links(url)
    elif "instagram.com" in url:
        return extract_instagram_links(url)
    else:
        return [["Unsupported", "URL type not supported"]]


if __name__ == "__main__":
    youtube_url = "https://www.youtube.com/@JolieChi"  # "https://www.youtube.com/@MayukiChou/about"
    instagram_url = "https://www.instagram.com/joliechi/"  # yu_zhen610 #mayukichou #haitaibear #mumumamagogo

    all_links = {
        "youtube": get_creator_links(youtube_url),
        "instagram": get_creator_links(instagram_url),
    }

    with open("creator_links.json", "w", encoding="utf-8") as f:
        json.dump(all_links, f, ensure_ascii=False, indent=2)
    print("Saved to creator_links.json")
